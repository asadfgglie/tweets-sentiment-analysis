{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn # intel device sklearn boosting\n",
    "patch_sklearn()\n",
    "import util\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from torch.cuda import is_available\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier # it take too much time to evaluation\n",
    "from sklearn.svm import LinearSVC # if training dataset is too large, we will not train in default SVC\n",
    "from sklearn.svm import SVC # only train in small training dataset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycontractions import Contractions\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.importance import MeanDecreaseImpurityImportanceEvaluator\n",
    "from optuna.visualization import plot_optimization_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = util.load_dataset()\n",
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "cont.load_models()\n",
    "dataset['text'] = dataset['text'].apply(util.clear_text)\n",
    "dataset['text'] = list(cont.expand_texts(dataset['text']))\n",
    "dataset['text'] = dataset['text'].apply(util.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_stopwords(text: str):\n",
    "    return ' '.join(t for t in str(text).split() if t not in stopwords.words('english'))\n",
    "dataset['text'] = dataset['text'].apply(clear_stopwords)\n",
    "dataset.to_csv('./dataset/clear_lemmatize_expand_texts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./dataset/clear_lemmatize_expand_texts.csv')\n",
    "dataset['text'] = dataset['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 281408)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(dataset['text'])\n",
    "print(cv.transform([dataset.iloc[-1]['text']]).toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 281408)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(dataset['text'])\n",
    "print(tfidf.transform([dataset.iloc[-1]['text']]).toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['len'] = dataset['text'].apply(lambda x: len(x))\n",
    "index = dataset[dataset['len'] == dataset['len'].max()].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test(cls, encoder, x=dataset['text'][index], times=10000):\n",
    "    print(cls.__class__.__name__, encoder.__class__.__name__)\n",
    "    a = []\n",
    "    for _ in range(times):\n",
    "        t1 = time.time()\n",
    "        cls.predict(encoder.transform([x]))\n",
    "        a.append(time.time() - t1)\n",
    "    print(np.mean(a))\n",
    "    return np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset['target'].to_numpy() // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: BaseEstimator, x, y=Y, batch=None, test_ratio=0.991, verbose=False, acc=True):\n",
    "    if verbose:\n",
    "        print(model.__class__.__name__)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio)\n",
    "    t1 = time.time()\n",
    "    model.fit(x_train[:batch], y_train[:batch])\n",
    "    t2 = time.time()\n",
    "    train_sc = None\n",
    "    test_sc = None\n",
    "    if acc:\n",
    "        train_sc = model.score(x_train[:batch], y_train[:batch])\n",
    "        test_sc = model.score(x_test[:batch], y_test[:batch])\n",
    "    if verbose and acc:\n",
    "        print('training time:', t2 - t1)\n",
    "        print('train acc:', train_sc)\n",
    "        print('test acc:', test_sc)\n",
    "        print('eval time:', time.time() - t2)\n",
    "    return train_sc, test_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=2024)\n",
    "xgb = XGBClassifier(random_state=2024)\n",
    "dtc = DecisionTreeClassifier(random_state=2024)\n",
    "lr = LogisticRegression(random_state=2024)\n",
    "gnb = GaussianNB()\n",
    "svc = SVC(random_state=2024)\n",
    "extree = ExtraTreesClassifier(random_state=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.transform(dataset['text'])\n",
    "train(rfc, X, verbose=True, acc=False)\n",
    "train(xgb, X, verbose=True, acc=False)\n",
    "train(dtc, X, verbose=True, acc=False)\n",
    "train(lr, X, verbose=True, acc=False)\n",
    "train(extree, X, verbose=True, acc=False)\n",
    "# train(gnb, X, verbose=True, acc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier CountVectorizer\n",
      "0.11365972511768341\n",
      "XGBClassifier CountVectorizer\n",
      "0.05128001439571381\n",
      "DecisionTreeClassifier CountVectorizer\n",
      "0.0007828950881958007\n",
      "LogisticRegression CountVectorizer\n",
      "0.0002606998682022095\n",
      "ExtraTreesClassifier CountVectorizer\n",
      "0.06992638602256775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06992638602256775"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_test(rfc, cv)\n",
    "inference_test(xgb, cv)\n",
    "inference_test(dtc, cv)\n",
    "inference_test(lr, cv)\n",
    "inference_test(extree, cv)\n",
    "# inference_test(gnb, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "XGBClassifier\n",
      "DecisionTreeClassifier\n",
      "LogisticRegression\n",
      "ExtraTreesClassifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidf.transform(dataset['text'])\n",
    "train(rfc, X, verbose=True, acc=False)\n",
    "train(xgb, X, verbose=True, acc=False)\n",
    "train(dtc, X, verbose=True, acc=False)\n",
    "train(lr, X, verbose=True, acc=False)\n",
    "train(extree, X, verbose=True, acc=False)\n",
    "# train(gnb, X, verbose=True, acc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier TfidfVectorizer\n",
      "0.06837882320880889\n",
      "XGBClassifier TfidfVectorizer\n",
      "0.05341551949977875\n",
      "DecisionTreeClassifier TfidfVectorizer\n",
      "0.005056393265724182\n",
      "LogisticRegression TfidfVectorizer\n",
      "0.004581047964096069\n",
      "ExtraTreesClassifier TfidfVectorizer\n",
      "0.07218445415496826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07218445415496826"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_test(rfc, tfidf)\n",
    "inference_test(xgb, tfidf)\n",
    "inference_test(dtc, tfidf)\n",
    "inference_test(lr, tfidf)\n",
    "inference_test(extree, tfidf)\n",
    "# inference_test(gnb, tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
